{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Packages ----------------\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import urllib.request as ur\n",
    "\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the dataframe function\n",
    "def create_listing_dataframe(Location,Beds,Baths,Parking,Laundry,Furnished,Price):\n",
    "    \n",
    "    '''\n",
    "    To create the dataframe from the arrays of data columns\n",
    "    \n",
    "    Input:\n",
    "    \n",
    "    Location: list of locations\n",
    "    Beds: list of beds\n",
    "    Baths: list of baths\n",
    "    Parking: list of parking\n",
    "    Laundry: list of laundry\n",
    "    Furnished: list of furnished\n",
    "    Price: list of prices\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    df: return the dataframe\n",
    "    \n",
    "    '''\n",
    "    df = pd.DataFrame(columns = ['Location','Beds','Baths','Parking','Laundry','Furnished','Price'])\n",
    "    df['Location'] = Location\n",
    "    df['Beds'] = Beds\n",
    "    df['Price'] = Price\n",
    "    df['Baths'] = Baths\n",
    "    #df['location_sector'] = location_sector\n",
    "    df['Parking'] = Parking\n",
    "    df['Laundry'] = Laundry\n",
    "    df['Furnished'] = Furnished\n",
    "    \n",
    "    convert_dict = {'Location': str, \n",
    "                'Beds': float,\n",
    "                'Baths': float,\n",
    "                'Price': float,\n",
    "                'Furnished': str,\n",
    "                'Parking': str,\n",
    "                'Laundry': str\n",
    "               } \n",
    "    df = df.astype(convert_dict) \n",
    "    \n",
    "    return (df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning the dataframe function\n",
    "def clean_listing_dataframe(df):\n",
    "    \n",
    "    '''\n",
    "    To clean the dataframe and get rid of bad data\n",
    "    \n",
    "    Input:\n",
    "    \n",
    "    df: Dataframe to clean\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    df: Returns the cleaned dataframe\n",
    "    \n",
    "    '''\n",
    "    df['Price'] = np.where((df['Price']<=1000) & (df['Beds']>2), df['Price']*df['Beds'], df['Price'])\n",
    "    \n",
    "    list_of_beds = [str(x) for x in range(9,21)]\n",
    "    list_of_beds.append('0')\n",
    "    df = df[~df.Beds.isin(list_of_beds)]\n",
    "    df = df[df['Price'] != '0']\n",
    "    df = df[df['Location'] != 'Null']\n",
    "    df = df[df['Baths'] != 'None']\n",
    "    df = df[df['Price'] != 'None']\n",
    "    df = df[df['Laundry'] != 'None']\n",
    "    df = df[df['Furnished'] != 'None']\n",
    "    df = df.dropna(how='any',axis=0) \n",
    "    \n",
    "    \n",
    "    df['Location'] = df['Location'].apply(lambda x: \"West\" if x == \"W\" else x)\n",
    "    df['Location']  = df['Location'].apply(lambda x: \"East\" if x == \"E\" else x)\n",
    "    df['Location']  = df['Location'].apply(lambda x: \"North\" if x == \"N\" else x)\n",
    "    df['Location']  = df['Location'].apply(lambda x: \"South\" if x == \"S\" else x)\n",
    "    df.columns = ['location','beds','baths','parking','laundry','furnished','price']\n",
    "    df.set_index('location', inplace=True)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database():\n",
    "    \n",
    "    '''\n",
    "    Create the database with the columns that are required to load in the data\n",
    "    \n",
    "    '''\n",
    "    import psycopg2\n",
    "    con = psycopg2.connect(database=\"postgres\", user=\"postgres\", password=\"password\", host=\"127.0.0.1\", port=\"5432\")\n",
    "    print(\"Database opened successfully\")\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.execute('''CREATE TABLE listings\n",
    "          (location VARCHAR(50)      NOT NULL,\n",
    "          beds     DECIMAL     NOT NULL,\n",
    "          baths    DECIMAL      NOT NULL,\n",
    "          parking  VARCHAR(50) NOT NULL,\n",
    "          laundry  VARCHAR(50) NOT NULL,\n",
    "          furnished  VARCHAR(50) NOT NULL,          \n",
    "          price     DECIMAL(50));''')\n",
    "    con.commit()\n",
    "    print(\"Table created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_database(df):\n",
    "    \n",
    "    '''\n",
    "    To load the dataframe to the database\n",
    "    \n",
    "    Input:\n",
    "    \n",
    "    df: The dataframe to load in\n",
    "\n",
    "    \n",
    "    '''\n",
    "    engine = create_engine('postgresql://postgres:password@127.0.0.1:5432/postgres')\n",
    "    df.to_sql('listing', engine, if_exists='append')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraping the listing function\n",
    "def scrape_listings():\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Code to scrape the data and convert it into lists\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}\n",
    "    direction = [\"North\",\"East\",\"West\",\"South\",\"E\",\"W\",\"N\",\"S\"]\n",
    "    pages = np.arange(0,160)\n",
    "    \n",
    "    location = []\n",
    "    beds = []\n",
    "    price = []\n",
    "    baths = []\n",
    "    location_sector = []\n",
    "    parking = []\n",
    "    laundry = []    \n",
    "    furnished = []\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    try:\n",
    "        for page in pages:\n",
    "            counter = counter+1\n",
    "            page_req = requests.get(\"https://offcampus.osu.edu/search-housing.aspx?page=\" + str(page) + \"&pricefrom=0&sort=1\", headers = headers,timeout =10)\n",
    "            url = \"https://offcampus.osu.edu/search-housing.aspx?page=\" + str(page) + \"&pricefrom=0&sort=1\"\n",
    "            soup = BeautifulSoup(page_req.text, \"html.parser\")\n",
    "            appartment_div = soup.find_all('div', class_='o-row__col o-row__col--6of12@md o-row__col--4of12@xl')\n",
    "            sleep(randint(2,10))\n",
    "\n",
    "            for container in appartment_div:\n",
    "                # location\n",
    "                name = container.h2.a.text\n",
    "                match = next((x for x in direction if x in name), \"Null\")\n",
    "                location.append(match)\n",
    "    \n",
    "                info = container.find_all(\"dd\")       \n",
    "    \n",
    "                ##bed\n",
    "                bed = int(info[1].text[0])\n",
    "                beds.append(bed)\n",
    "    \n",
    "                ##Baths\n",
    "                bath = int(info[2].text[0])\n",
    "                baths.append(bath)\n",
    "            \n",
    "        \n",
    "          \n",
    "        \n",
    "                possible_links = soup.find_all(lambda tag: tag.name == 'div' and \n",
    "                                tag.get('class') == ['c-propertycard__info'])\n",
    "        \n",
    "                listings = []\n",
    "        \n",
    "  \n",
    "            \n",
    "                for div in possible_links:\n",
    "                    listings.append(div.a['href'])\n",
    "            \n",
    "                for list_e in listings:\n",
    "                    page = requests.get(\"https://offcampus.osu.edu/\" + str(list_e), headers = headers, timeout =10)\n",
    "                    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "                    ul = soup.select('div[class=\"o-row__col o-row__col--6of12@lg\"] li', recursive=True)\n",
    "                    lis_e = []\n",
    "                    for li in ul:\n",
    "                        lis = []\n",
    "                        lis.append(li.contents)\n",
    "                        lis_e.extend(lis)\n",
    "        \n",
    "                    \n",
    "                    pattern = re.compile('(?<=University District:)\\s(\\w+)')\n",
    "                    matches = pattern.search(str(lis_e))\n",
    "                    if matches:\n",
    "                        location_sector.append(matches.group(1))\n",
    "                    else:\n",
    "                        location_sector.append(\"None\")\n",
    "                    pattern = re.compile('(?<=Off-street Parking:).*\\s.*(Yes|No)')\n",
    "                    matches = pattern.search(str(lis_e))\n",
    "                    if matches:\n",
    "                        parking.append(matches.group(1))\n",
    "                    else:\n",
    "                        parking.append(\"None\")\n",
    "                    pattern = re.compile('Laundry facilities in the (unit|building)')\n",
    "                    matches = pattern.search(str(lis_e))\n",
    "                    if matches:\n",
    "                        laundry.append(matches.group(1))\n",
    "                    else:\n",
    "                        laundry.append(\"None\")\n",
    "                    pattern = re.compile('(?<=Furnished).*(Yes|No)')\n",
    "                    matches = pattern.search(str(lis_e))\n",
    "                    if matches:\n",
    "                        furnished.append(matches.group(1))\n",
    "                    else:\n",
    "                        furnished.append(\"None\")\n",
    "    \n",
    "                    c = lis_e[1][1].split()\n",
    "                    if len(c) ==  1:\n",
    "                        price.append(float(c[0].replace('$','').replace(\",\",'')))\n",
    "                    else:\n",
    "                        num_1 = float(c[2].replace('$','').replace(\",\",''))\n",
    "                        price.append((num_1))\n",
    "                \n",
    "    except requests.exceptions.Timeout:\n",
    "                print(\"Timeout occurred\\n\")\n",
    "                print(\"Scrapping done\")\n",
    "                \n",
    "                \n",
    "    print(\"---------------Done Scrapping------------------\")\n",
    "    print(\"---------------Loading data to pandas----------\")\n",
    "    df = create_listing_dataframe(location,beds,baths,parking,laundry,furnished,price)\n",
    "    print(\"---------------Cleaning the data----------------\")\n",
    "    df =  clean_listing_dataframe(df)\n",
    "    print(\"---------------Loading the data to the database------------\")\n",
    "    load_data_to_database(df)\n",
    "     \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_listings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
